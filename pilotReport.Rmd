---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---


# Report Details


```{r}
articleID <- "EXT_11-5-2015" # insert the article ID code here
reportType <- "pilot" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "George Kachergis" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- NA # # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- NA # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/19/18", format = "%m/%d/%y") # insert the pilot's start date in US format 
copilotStartDate <- NA # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

Experiment 1 included 269 participants who were asked to recall and write about their behavior in an event in their life. 
As a manipulation 
Participants were randomly assigned to a condition in the 2 (behavior type: authentic vs. inauthentic) × 2 (event type: general vs. unrelated to lying) between-subjects design. 
After writing the essay, their feelings of moral self-regard  (10 items) and impurity (3 items) were self-reported on a scale of 1 to 7.
Two participants were excluded for not writing an essay. 

------

#### Target outcomes: 

The target outcomes for Experiment 1 were:

> Similar 2 × 2 ANOVAs using impurity and moral self-regard as dependent measures also revealed only a significant main effect of type of behavior. Participants in the inauthentic-behavior condition reported greater feelings of impurity (M = 3.56, SD = 1.86, 95% CI = [3.30, 3.85]) and lower moral self-regard (M = 2.90, SD = 1.50, 95% CI = [2.61, 3.16]) than did participants in the authentic-behavior condition (impurity: M = 1.51, SD = 1.29, 95% CI = [1.25, 1.78]; moral self-regard: M = 4.99, SD = 1.68, 95% CI = [4.72, 5.26]), F(1, 263) = 111.06, p < .001, ηp 2 = .30, and F(1, 263) = 115.25, p < .001, ηp2 = .31, respectively. (from Gino et al. p. 986)

------

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

[PILOT/COPILOT Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(ez)
```


```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
dat <- read_spss("data/data_Experiment_1.sav")
```

# Step 3: Tidy data

```{r}
tdat <- dat %>% 
  filter(failed_essay==F)

tdat$subject = as.factor(1:nrow(tdat))
  
```

# Step 4: Run analysis

## Pre-processing

```{r}
```

## Descriptive statistics

```{r}
Reported_Alpha = c(.965, .94)
moral = psych::alpha(tdat[,paste("moralSR_",1:10, sep='')])$total 
impurity = psych::alpha(tdat[,paste("Impurity_",1:3, sep='')])$total # .94 vs. reported .94
Obtained_Alpha = round(c(moral$raw_alpha, impurity$raw_alpha), digits = 3)
allAlphas <- cbind(Obtained_Alpha, Reported_Alpha)
knitr::kable(allAlphas, caption = "Alpha")

Reported_moralSR <- data.frame(Reported_Average = c(2.90, 4.99), 
                               Reported_SD = c(1.50, 1.68),
                               Reported_CI_lower = c(2.61, 4.72),
                               Reported_CI_upper = c(3.16, 5.26))
obtained_moralSR <- tdat %>%
  group_by(authenticity) %>%
  summarise(Obtained_Average = round((mean(moral_self_regard)), digits = 2), 
            Obtained_SD = round((sd(moral_self_regard)), digits = 2))


allMoralSR <- bind_cols(obtained_moralSR, Reported_moralSR)
knitr::kable(allMoralSR, caption = "Moral Self-Regard by Authenticity")

Reported_impurity <- data.frame(Reported_impurity = c(3.56, 1.51), 
                                Reported_SD=c(1.86, 1.29),
                                Reported_CI_lower = c(3.30, 1.25),
                                Reported_CI_upper = c(3.85, 1.78))
obtained_impurity <- tdat %>%
  group_by(authenticity) %>%
  summarise(Obtained_Average = round((mean(impurity)), digits = 2),
            Obtained_SD = round((sd(impurity)), digits = 2))
allImpurity <- bind_cols(obtained_impurity, Reported_impurity)
knitr::kable(allImpurity, caption = "Impurity by Authenticity")

reportObject <- reproCheck(reportedValue = '.965', obtainedValue = moral$raw_alpha, valueType = 'other')
reportObject <- reproCheck(reportedValue = '.94', obtainedValue = impurity$raw_alpha, valueType = 'other')

reportObject <- reproCheck(reportedValue = '2.90', obtainedValue = allMoralSR[1,]$Obtained_Average, valueType = 'mean')
reportObject <- reproCheck(reportedValue = '4.99', obtainedValue = allMoralSR[2,]$Obtained_Average, valueType = 'mean')

reportObject <- reproCheck(reportedValue = '3.56', obtainedValue = allImpurity[1,]$Obtained_Average, valueType = 'mean')
reportObject <- reproCheck(reportedValue = '1.51', obtainedValue = allImpurity[2,]$Obtained_Average, valueType = 'mean')

reportObject <- reproCheck(reportedValue = '1.50', obtainedValue = allMoralSR[1,]$Obtained_SD, valueType = 'sd')
reportObject <- reproCheck(reportedValue = '1.68', obtainedValue = allMoralSR[2,]$Obtained_SD, valueType = 'sd')

reportObject <- reproCheck(reportedValue = '1.86', obtainedValue = allImpurity[1,]$Obtained_SD, valueType = 'sd')
reportObject <- reproCheck(reportedValue = '1.29', obtainedValue = allImpurity[2,]$Obtained_SD, valueType = 'sd')

```

INSUFFICIENT INFORMATION ERROR

All of the means and standard deviations match, but after trying to reproduce the reported 95% confidence intervals which were not included in the SPSS syntax, I was unable to get quite the same CIs (though within a few percent; see below).

```{r}
get_95CIs <- function(x) {
  sem = sd(x) / sqrt(length(x)) 
  return(c(mean(x)-2*sem, mean(x)+2*sem))
}

obtained_CIs_impure_auth = get_95CIs( subset(tdat, authenticity==1)$impurity ) 
obtained_CIs_impure_inauth = get_95CIs( subset(tdat, authenticity==0)$impurity ) 

obtained_CIs_moralSR_auth = get_95CIs( subset(tdat, authenticity==1)$moral_self_regard ) 
obtained_CIs_moralSR_inauth = get_95CIs( subset(tdat, authenticity==0)$moral_self_regard ) 

# trying 95% CIs another way, get very similar (and different than reported values)
unlist(t.test(subset(tdat, authenticity==1)$impurity)$conf.int)[1:2] 
unlist(t.test(subset(tdat, authenticity==0)$impurity)$conf.int)[1:2] 

unlist(t.test(subset(tdat, authenticity==1)$moral_self_regard)$conf.int)[1:2] 
unlist(t.test(subset(tdat, authenticity==0)$moral_self_regard)$conf.int)[1:2] 

```

Let's compare and record these values.
```{r}
# moral SR
reportObject <- reproCheck(reportedValue = '4.72', obtainedValue = obtained_CIs_moralSR_auth[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = '5.27', obtainedValue = obtained_CIs_moralSR_auth[2], valueType = 'ci')

reportObject <- reproCheck(reportedValue = '2.61', obtainedValue = obtained_CIs_moralSR_inauth[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = '3.16', obtainedValue = obtained_CIs_moralSR_inauth[2], valueType = 'ci')

# impure
reportObject <- reproCheck(reportedValue = '1.25', obtainedValue = obtained_CIs_impure_auth[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = '1.78', obtainedValue = obtained_CIs_impure_auth[2], valueType = 'ci')

reportObject <- reproCheck(reportedValue = '3.30', obtainedValue = obtained_CIs_impure_inauth[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = '3.85', obtainedValue = obtained_CIs_impure_inauth[2], valueType = 'ci')

```

## Inferential statistics

```{r}
# impurity ANOVA
m = ezANOVA(tdat, dv = .(impurity), wid = .(subject), between = .(authenticity, general_type))

# Dfn
reportObject <- reproCheck(reportedValue = "1", obtainedValue = m$ANOVA$DFn[1], valueType = 'df')
# Dfd
reportObject <- reproCheck(reportedValue = "263", obtainedValue = m$ANOVA$DFd[1], valueType = 'df')
# F
reportObject <- reproCheck(reportedValue = "111.06", obtainedValue = m$ANOVA$F[1], valueType = 'F')
# p
reportObject <- reproCheck(reportedValue = "<.001", obtainedValue = m$ANOVA$p[1], valueType = 'p', eyeballCheck = T)
# ηp2 ?
reportObject <- reproCheck(reportedValue = ".30", obtainedValue = m$ANOVA$ges[1], valueType = 'pes')


# moral self-regard ANOVA
m_imp = ezANOVA(tdat, dv = .(moral_self_regard), wid = .(subject), between = .(authenticity, general_type))

# Dfn
reportObject <- reproCheck(reportedValue = "1", obtainedValue = m_imp$ANOVA$DFn[1], valueType = 'df')
# Dfd
reportObject <- reproCheck(reportedValue = "263", obtainedValue = m_imp$ANOVA$DFd[1], valueType = 'df')
# F
reportObject <- reproCheck(reportedValue = "115.25", obtainedValue = m_imp$ANOVA$F[1], valueType = 'F')
# p
reportObject <- reproCheck(reportedValue = "<.001", obtainedValue = m_imp$ANOVA$p[1], valueType = 'p', eyeballCheck = T)
# ηp2 ?
reportObject <- reproCheck(reportedValue = ".31", obtainedValue = m_imp$ANOVA$ges[1], valueType = 'pes')

```

# Step 5: Conclusion

This reproducibility check was largely a success: all of the descriptive and inferential statistics were reproduced with only minor numerical differences. The only trouble was that the authors' included SPSS analysis did not show how the 95% confidence intervals reported in the paper were calculated, and I could not 

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 1 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- 0 # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- 1 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- 0 # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- 0 # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- 1 # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- 0 # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- 1 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- FALSE # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

[PILOT/COPILOT DO NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
